{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "https://wikidocs.net/64703"
      ],
      "metadata": {
        "id": "oxdYojJDgu2b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7-3 문자 단위 (RNN)"
      ],
      "metadata": {
        "id": "HFNKyfT1g22W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. 문자 단위 RNN (Char RNN)"
      ],
      "metadata": {
        "id": "Y4pGiRgJg56c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "YwtcgDZNg8z7"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1) 훈련 데이터 전처리하기\n",
        "입력 데이터와 레이블 데이터에 대해서 문자 집합을 만듦"
      ],
      "metadata": {
        "id": "eR91jNQvhDsY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_str = 'apple'\n",
        "label_str = 'pple!'\n",
        "char_vocab = sorted(list(set(input_str+label_str)))\n",
        "vocab_size = len(char_vocab)\n",
        "print ('문자 집합의 크기 : {}'.format(vocab_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_S9K7AbhLQ9",
        "outputId": "518685e0-d5be-4d3a-9b65-ed2fffeaf009"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문자 집합의 크기 : 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "현재 문자 집합에는 총 5개의 문자가 있음\n",
        "- 입력은 원-핫 벡터를 사용할 것"
      ],
      "metadata": {
        "id": "iMOAGVsrhUIQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = vocab_size # 입력의 크기는 문자 집합의 크기\n",
        "hidden_size = 5\n",
        "output_size = 5\n",
        "learning_rate = 0.1"
      ],
      "metadata": {
        "id": "kMx7t5vFhMy4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "문자 집합에 고유한 정수를 부여"
      ],
      "metadata": {
        "id": "rAj04RNDhZ10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "char_to_index = dict((c, i) for i, c in enumerate(char_vocab)) # 문자에 고유한 정수 인덱스 부여\n",
        "print(char_to_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBVEO_rdhPZ3",
        "outputId": "3aea0746-0407-41c5-de52-1da164d98204"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'!': 0, 'a': 1, 'e': 2, 'l': 3, 'p': 4}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "정수로부터 문자를 얻을 수 있는 index_to_char"
      ],
      "metadata": {
        "id": "BGrcgktohdrR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index_to_char={}\n",
        "for key, value in char_to_index.items():\n",
        "    index_to_char[value] = key\n",
        "print(index_to_char)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrODAVdShikc",
        "outputId": "ccaa1730-4c1f-4d2d-e123-46ad1f1d27a5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: '!', 1: 'a', 2: 'e', 3: 'l', 4: 'p'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "입력 데이터와 레이블 데이터의 각 문자들을 정수로 맵핑"
      ],
      "metadata": {
        "id": "Krpa3xZJhi9R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_data = [char_to_index[c] for c in input_str]\n",
        "y_data = [char_to_index[c] for c in label_str]\n",
        "print(x_data)\n",
        "print(y_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tw7F_MVqhmjj",
        "outputId": "55b9a1d3-59bd-445e-bd20-87b80ff96e50"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 4, 4, 3, 2]\n",
            "[4, 4, 3, 2, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "파이토치의 nn.RNN()은 기본적으로 3차원 텐서를 입력 받음"
      ],
      "metadata": {
        "id": "0fweb9LGhoAB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 배치 차원 추가\n",
        "# 텐서 연산인 unsqueeze(0)를 통해 해결할 수도 있었음.\n",
        "x_data = [x_data]\n",
        "y_data = [y_data]\n",
        "print(x_data)\n",
        "print(y_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvTWpiUKhu_a",
        "outputId": "f11807b1-1c14-4252-d19a-99f2214ddcfd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 4, 4, 3, 2]]\n",
            "[[4, 4, 3, 2, 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "입력 시퀀스의 각 문자들을 원-핫 벡터로 바꿈"
      ],
      "metadata": {
        "id": "RiXDLpyvhwez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_one_hot = [np.eye(vocab_size)[x] for x in x_data]\n",
        "print(x_one_hot)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhnKYK60hzOs",
        "outputId": "89314e3b-ff23-4c56-f565-0dbdd4471554"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([[0., 1., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 1.],\n",
            "       [0., 0., 0., 0., 1.],\n",
            "       [0., 0., 0., 1., 0.],\n",
            "       [0., 0., 1., 0., 0.]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "입력 데이터와 레이블 데이터를 텐서로 바꿈"
      ],
      "metadata": {
        "id": "_MxXyO6Zh0aK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.FloatTensor(x_one_hot)\n",
        "Y = torch.LongTensor(y_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBLpF-IJh3L6",
        "outputId": "def53254-69fa-46e3-a3ab-e2217bf1e99f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-c1bfbd518a63>:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
            "  X = torch.FloatTensor(x_one_hot)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "각 텐서의 크기를 확인하기"
      ],
      "metadata": {
        "id": "T02dV6MLh4SZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('훈련 데이터의 크기 : {}'.format(X.shape))\n",
        "print('레이블의 크기 : {}'.format(Y.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipZpnER-h53K",
        "outputId": "703e4237-fa90-4f72-9f18-39b691ce7716"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련 데이터의 크기 : torch.Size([1, 5, 5])\n",
            "레이블의 크기 : torch.Size([1, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2) 모델 구현하기"
      ],
      "metadata": {
        "id": "IO5E9p4Qh7BM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RNN 모델을 구현"
      ],
      "metadata": {
        "id": "EBhXDoxxh-GT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(Net, self).__init__()\n",
        "        self.rnn = torch.nn.RNN(input_size, hidden_size, batch_first=True) # RNN 셀 구현\n",
        "        self.fc = torch.nn.Linear(hidden_size, output_size, bias=True) # 출력층 구현\n",
        "\n",
        "    def forward(self, x): # 구현한 RNN 셀과 출력층을 연결\n",
        "        x, _status = self.rnn(x)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "cG91BUE_iAUX"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "클래스로 정의한 모델을 net에 저장"
      ],
      "metadata": {
        "id": "hEDmGhLtiBXb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net = Net(input_size, hidden_size, output_size)"
      ],
      "metadata": {
        "id": "WyGjYz3PiD7h"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "입력된 모델에 입력을 넣어서 출력의 크기를 확인"
      ],
      "metadata": {
        "id": "l5mW7XafiE6L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = net(X)\n",
        "print(outputs.shape) # 3차원 텐서"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIeVwPl4iHYp",
        "outputId": "0ee056bd-a666-4f27-b1b5-dfe869ad2cae"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 5, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "배치 차원과 시점 차원을 하나로 만듦"
      ],
      "metadata": {
        "id": "fDOAFhZNiIn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(outputs.view(-1, input_size).shape) # 2차원 텐서로 변환"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9heRxLtiNnV",
        "outputId": "cbd042b1-1676-4197-cafa-59f6c6b51857"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "레이블 데이터의 크기"
      ],
      "metadata": {
        "id": "NdGQYLnxiOrH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(Y.shape)\n",
        "print(Y.view(-1).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxYjN64oiRdS",
        "outputId": "2edce699-2def-4d00-f93f-1e50202d71f5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 5])\n",
            "torch.Size([5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "옵티마이저와 손실 함수를 정의"
      ],
      "metadata": {
        "id": "LBASQez-iSx_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), learning_rate)"
      ],
      "metadata": {
        "id": "sU8kK8d9iW7q"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "100 에포크 학습"
      ],
      "metadata": {
        "id": "CxxEZNebiX8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(100):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = net(X)\n",
        "    loss = criterion(outputs.view(-1, input_size), Y.view(-1)) # view를 하는 이유는 Batch 차원 제거를 위해\n",
        "    loss.backward() # 기울기 계산\n",
        "    optimizer.step() # 아까 optimizer 선언 시 넣어둔 파라미터 업데이트\n",
        "\n",
        "    # 아래 세 줄은 모델이 실제 어떻게 예측했는지를 확인하기 위한 코드.\n",
        "    result = outputs.data.numpy().argmax(axis=2) # 최종 예측값인 각 time-step 별 5차원 벡터에 대해서 가장 높은 값의 인덱스를 선택\n",
        "    result_str = ''.join([index_to_char[c] for c in np.squeeze(result)])\n",
        "    print(i, \"loss: \", loss.item(), \"prediction: \", result, \"true Y: \", y_data, \"prediction str: \", result_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0c8Z0ioiZMn",
        "outputId": "a26ee0cd-5e8d-40c6-db6e-cabe7c7682b5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 loss:  1.6422455310821533 prediction:  [[0 0 0 0 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  !!!!!\n",
            "1 loss:  1.3725173473358154 prediction:  [[4 4 4 4 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pppp!\n",
            "2 loss:  1.1958515644073486 prediction:  [[4 4 4 4 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pppp!\n",
            "3 loss:  1.00653076171875 prediction:  [[4 4 4 4 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pppp!\n",
            "4 loss:  0.8131033182144165 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "5 loss:  0.6072437763214111 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "6 loss:  0.4309268891811371 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "7 loss:  0.29342225193977356 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "8 loss:  0.1973179280757904 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "9 loss:  0.13486412167549133 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "10 loss:  0.09429754316806793 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "11 loss:  0.06735457479953766 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "12 loss:  0.049047790467739105 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "13 loss:  0.03614335507154465 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "14 loss:  0.02654220722615719 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "15 loss:  0.019167950376868248 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "16 loss:  0.013793488033115864 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "17 loss:  0.010291135869920254 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "18 loss:  0.008108614943921566 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "19 loss:  0.006651841104030609 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "20 loss:  0.005582091864198446 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "21 loss:  0.0047490475699305534 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "22 loss:  0.004083194769918919 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "23 loss:  0.0035447492264211178 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "24 loss:  0.0031063922215253115 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "25 loss:  0.0027474667876958847 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "26 loss:  0.0024516177363693714 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "27 loss:  0.0022061176132410765 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "28 loss:  0.002000620122998953 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "29 loss:  0.0018270701402798295 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "30 loss:  0.0016791382804512978 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "31 loss:  0.001551749766804278 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "32 loss:  0.0014409662690013647 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "33 loss:  0.001343750162050128 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "34 loss:  0.0012578228488564491 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "35 loss:  0.0011814278550446033 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "36 loss:  0.0011131643550470471 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "37 loss:  0.0010520827490836382 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "38 loss:  0.0009971136460080743 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "39 loss:  0.000947806634940207 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "40 loss:  0.0009034486720338464 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "41 loss:  0.000863635737914592 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "42 loss:  0.0008278212626464665 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "43 loss:  0.0007957200286909938 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "44 loss:  0.0007667847676202655 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "45 loss:  0.0007408253732137382 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "46 loss:  0.000717461109161377 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "47 loss:  0.0006964065250940621 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "48 loss:  0.0006774234352633357 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "49 loss:  0.000660297810100019 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "50 loss:  0.0006446724873967469 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "51 loss:  0.0006305474671535194 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "52 loss:  0.0006176130846142769 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "53 loss:  0.0006058696890249848 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "54 loss:  0.0005950549966655672 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "55 loss:  0.0005850738962180912 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "56 loss:  0.0005758072948083282 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "57 loss:  0.0005672076949849725 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "58 loss:  0.0005592033267021179 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "59 loss:  0.0005517231184057891 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "60 loss:  0.000544671609532088 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "61 loss:  0.0005379534559324384 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "62 loss:  0.0005316402530297637 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "63 loss:  0.0005256367148831487 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "64 loss:  0.0005199190345592797 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "65 loss:  0.0005144632305018604 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "66 loss:  0.0005092219216749072 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "67 loss:  0.0005042426055297256 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "68 loss:  0.000499382265843451 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "69 loss:  0.0004947364213876426 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "70 loss:  0.000490281090606004 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "71 loss:  0.0004859686887357384 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "72 loss:  0.0004817515145987272 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "73 loss:  0.00047774877748452127 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "74 loss:  0.0004738889983855188 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "75 loss:  0.0004701006109826267 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "76 loss:  0.00046645518159493804 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "77 loss:  0.00046290503814816475 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "78 loss:  0.000459497794508934 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "79 loss:  0.00045616208808496594 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "80 loss:  0.00045287393731996417 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "81 loss:  0.00044972877367399633 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "82 loss:  0.0004466074169613421 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "83 loss:  0.0004436051531229168 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "84 loss:  0.0004406028310768306 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "85 loss:  0.0004377912264317274 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "86 loss:  0.0004349556693341583 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "87 loss:  0.00043219164945185184 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "88 loss:  0.0004294990503694862 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "89 loss:  0.0004267826152499765 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "90 loss:  0.0004241138813085854 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "91 loss:  0.0004215880180709064 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "92 loss:  0.00041899076313711703 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "93 loss:  0.0004165602440480143 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "94 loss:  0.00041405815863981843 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "95 loss:  0.0004116276395507157 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "96 loss:  0.0004092447052244097 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "97 loss:  0.00040681412792764604 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "98 loss:  0.00040457415161654353 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "99 loss:  0.00040221508243121207 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. 더 많은 데이터로 학습한 문자 단위 RNN"
      ],
      "metadata": {
        "id": "2De1OgPziawT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "iNsR_5VrifBH"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1) 훈련 데이터 전처리하기"
      ],
      "metadata": {
        "id": "xy9JwrxPiv63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = (\"if you want to build a ship, don't drum up people together to \"\n",
        "            \"collect wood and don't assign them tasks and work, but rather \"\n",
        "            \"teach them to long for the endless immensity of the sea.\")"
      ],
      "metadata": {
        "id": "5qDh9UzoizTh"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "문서 집합을 생성하고, 각 문자에 고유한 정수를 부여함"
      ],
      "metadata": {
        "id": "bfnB7UA0i02m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "char_set = list(set(sentence)) # 중복을 제거한 문자 집합 생성\n",
        "char_dic = {c: i for i, c in enumerate(char_set)} # 각 문자에 정수 인코딩"
      ],
      "metadata": {
        "id": "KG6nC09ii3sW"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(char_dic) # 공백도 여기서는 하나의 원소"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4SNBzbwi4yL",
        "outputId": "eb3fb70a-f8a5-4c53-e391-d0990e036b49"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'r': 0, 'k': 1, ' ': 2, 'w': 3, 'm': 4, 'a': 5, 'd': 6, 'p': 7, 'y': 8, 'b': 9, 'n': 10, 'u': 11, 'g': 12, '.': 13, 'h': 14, \"'\": 15, 't': 16, 'o': 17, 'f': 18, ',': 19, 'i': 20, 'c': 21, 's': 22, 'e': 23, 'l': 24}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "각 문자에 정수가 부여됨"
      ],
      "metadata": {
        "id": "I-TH9hIEi6Jn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dic_size = len(char_dic)\n",
        "print('문자 집합의 크기 : {}'.format(dic_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BAbp0VVi8yJ",
        "outputId": "bf8d445e-658f-4064-f994-81206467b3b2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문자 집합의 크기 : 25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "하이퍼파라미터를 설정\n",
        "- hidden_size를 입력의 크기와 동일하게 줌\n",
        "\n",
        "sequence_length 변수 선언하기"
      ],
      "metadata": {
        "id": "bo3FUujHi9y6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 하이퍼파라미터 설정\n",
        "hidden_size = dic_size\n",
        "sequence_length = 10  # 임의 숫자 지정\n",
        "learning_rate = 0.1"
      ],
      "metadata": {
        "id": "6feEPgY_jMyk"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "sequence_length 값이 10의 단위로 샘플들을 잘라서 데이터를 만든느 모습"
      ],
      "metadata": {
        "id": "yDNj3-FvjODv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 구성\n",
        "x_data = []\n",
        "y_data = []\n",
        "\n",
        "for i in range(0, len(sentence) - sequence_length):\n",
        "    x_str = sentence[i:i + sequence_length]\n",
        "    y_str = sentence[i + 1: i + sequence_length + 1]\n",
        "    print(i, x_str, '->', y_str)\n",
        "\n",
        "    x_data.append([char_dic[c] for c in x_str])  # x str to index\n",
        "    y_data.append([char_dic[c] for c in y_str])  # y str to index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPIJFhb6jSro",
        "outputId": "320f0660-2e25-419a-cc21-a4310e6dc61b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 if you wan -> f you want\n",
            "1 f you want ->  you want \n",
            "2  you want  -> you want t\n",
            "3 you want t -> ou want to\n",
            "4 ou want to -> u want to \n",
            "5 u want to  ->  want to b\n",
            "6  want to b -> want to bu\n",
            "7 want to bu -> ant to bui\n",
            "8 ant to bui -> nt to buil\n",
            "9 nt to buil -> t to build\n",
            "10 t to build ->  to build \n",
            "11  to build  -> to build a\n",
            "12 to build a -> o build a \n",
            "13 o build a  ->  build a s\n",
            "14  build a s -> build a sh\n",
            "15 build a sh -> uild a shi\n",
            "16 uild a shi -> ild a ship\n",
            "17 ild a ship -> ld a ship,\n",
            "18 ld a ship, -> d a ship, \n",
            "19 d a ship,  ->  a ship, d\n",
            "20  a ship, d -> a ship, do\n",
            "21 a ship, do ->  ship, don\n",
            "22  ship, don -> ship, don'\n",
            "23 ship, don' -> hip, don't\n",
            "24 hip, don't -> ip, don't \n",
            "25 ip, don't  -> p, don't d\n",
            "26 p, don't d -> , don't dr\n",
            "27 , don't dr ->  don't dru\n",
            "28  don't dru -> don't drum\n",
            "29 don't drum -> on't drum \n",
            "30 on't drum  -> n't drum u\n",
            "31 n't drum u -> 't drum up\n",
            "32 't drum up -> t drum up \n",
            "33 t drum up  ->  drum up p\n",
            "34  drum up p -> drum up pe\n",
            "35 drum up pe -> rum up peo\n",
            "36 rum up peo -> um up peop\n",
            "37 um up peop -> m up peopl\n",
            "38 m up peopl ->  up people\n",
            "39  up people -> up people \n",
            "40 up people  -> p people t\n",
            "41 p people t ->  people to\n",
            "42  people to -> people tog\n",
            "43 people tog -> eople toge\n",
            "44 eople toge -> ople toget\n",
            "45 ople toget -> ple togeth\n",
            "46 ple togeth -> le togethe\n",
            "47 le togethe -> e together\n",
            "48 e together ->  together \n",
            "49  together  -> together t\n",
            "50 together t -> ogether to\n",
            "51 ogether to -> gether to \n",
            "52 gether to  -> ether to c\n",
            "53 ether to c -> ther to co\n",
            "54 ther to co -> her to col\n",
            "55 her to col -> er to coll\n",
            "56 er to coll -> r to colle\n",
            "57 r to colle ->  to collec\n",
            "58  to collec -> to collect\n",
            "59 to collect -> o collect \n",
            "60 o collect  ->  collect w\n",
            "61  collect w -> collect wo\n",
            "62 collect wo -> ollect woo\n",
            "63 ollect woo -> llect wood\n",
            "64 llect wood -> lect wood \n",
            "65 lect wood  -> ect wood a\n",
            "66 ect wood a -> ct wood an\n",
            "67 ct wood an -> t wood and\n",
            "68 t wood and ->  wood and \n",
            "69  wood and  -> wood and d\n",
            "70 wood and d -> ood and do\n",
            "71 ood and do -> od and don\n",
            "72 od and don -> d and don'\n",
            "73 d and don' ->  and don't\n",
            "74  and don't -> and don't \n",
            "75 and don't  -> nd don't a\n",
            "76 nd don't a -> d don't as\n",
            "77 d don't as ->  don't ass\n",
            "78  don't ass -> don't assi\n",
            "79 don't assi -> on't assig\n",
            "80 on't assig -> n't assign\n",
            "81 n't assign -> 't assign \n",
            "82 't assign  -> t assign t\n",
            "83 t assign t ->  assign th\n",
            "84  assign th -> assign the\n",
            "85 assign the -> ssign them\n",
            "86 ssign them -> sign them \n",
            "87 sign them  -> ign them t\n",
            "88 ign them t -> gn them ta\n",
            "89 gn them ta -> n them tas\n",
            "90 n them tas ->  them task\n",
            "91  them task -> them tasks\n",
            "92 them tasks -> hem tasks \n",
            "93 hem tasks  -> em tasks a\n",
            "94 em tasks a -> m tasks an\n",
            "95 m tasks an ->  tasks and\n",
            "96  tasks and -> tasks and \n",
            "97 tasks and  -> asks and w\n",
            "98 asks and w -> sks and wo\n",
            "99 sks and wo -> ks and wor\n",
            "100 ks and wor -> s and work\n",
            "101 s and work ->  and work,\n",
            "102  and work, -> and work, \n",
            "103 and work,  -> nd work, b\n",
            "104 nd work, b -> d work, bu\n",
            "105 d work, bu ->  work, but\n",
            "106  work, but -> work, but \n",
            "107 work, but  -> ork, but r\n",
            "108 ork, but r -> rk, but ra\n",
            "109 rk, but ra -> k, but rat\n",
            "110 k, but rat -> , but rath\n",
            "111 , but rath ->  but rathe\n",
            "112  but rathe -> but rather\n",
            "113 but rather -> ut rather \n",
            "114 ut rather  -> t rather t\n",
            "115 t rather t ->  rather te\n",
            "116  rather te -> rather tea\n",
            "117 rather tea -> ather teac\n",
            "118 ather teac -> ther teach\n",
            "119 ther teach -> her teach \n",
            "120 her teach  -> er teach t\n",
            "121 er teach t -> r teach th\n",
            "122 r teach th ->  teach the\n",
            "123  teach the -> teach them\n",
            "124 teach them -> each them \n",
            "125 each them  -> ach them t\n",
            "126 ach them t -> ch them to\n",
            "127 ch them to -> h them to \n",
            "128 h them to  ->  them to l\n",
            "129  them to l -> them to lo\n",
            "130 them to lo -> hem to lon\n",
            "131 hem to lon -> em to long\n",
            "132 em to long -> m to long \n",
            "133 m to long  ->  to long f\n",
            "134  to long f -> to long fo\n",
            "135 to long fo -> o long for\n",
            "136 o long for ->  long for \n",
            "137  long for  -> long for t\n",
            "138 long for t -> ong for th\n",
            "139 ong for th -> ng for the\n",
            "140 ng for the -> g for the \n",
            "141 g for the  ->  for the e\n",
            "142  for the e -> for the en\n",
            "143 for the en -> or the end\n",
            "144 or the end -> r the endl\n",
            "145 r the endl ->  the endle\n",
            "146  the endle -> the endles\n",
            "147 the endles -> he endless\n",
            "148 he endless -> e endless \n",
            "149 e endless  ->  endless i\n",
            "150  endless i -> endless im\n",
            "151 endless im -> ndless imm\n",
            "152 ndless imm -> dless imme\n",
            "153 dless imme -> less immen\n",
            "154 less immen -> ess immens\n",
            "155 ess immens -> ss immensi\n",
            "156 ss immensi -> s immensit\n",
            "157 s immensit ->  immensity\n",
            "158  immensity -> immensity \n",
            "159 immensity  -> mmensity o\n",
            "160 mmensity o -> mensity of\n",
            "161 mensity of -> ensity of \n",
            "162 ensity of  -> nsity of t\n",
            "163 nsity of t -> sity of th\n",
            "164 sity of th -> ity of the\n",
            "165 ity of the -> ty of the \n",
            "166 ty of the  -> y of the s\n",
            "167 y of the s ->  of the se\n",
            "168  of the se -> of the sea\n",
            "169 of the sea -> f the sea.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> 170개의 샘플이 생성\n",
        "\n",
        "각 샘플의 각 문자들은 고유한 정수로 인코딩 된 상태"
      ],
      "metadata": {
        "id": "K7TXKI5IjUFE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_data[0])\n",
        "print(y_data[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6blQL-ijc3b",
        "outputId": "b08b8d27-755d-4dc0-cf1b-9f426ec749f9"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[20, 18, 2, 8, 17, 11, 2, 3, 5, 10]\n",
            "[18, 2, 8, 17, 11, 2, 3, 5, 10, 16]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "한 칸씩 쉬프트 된 시퀀스가 정상적으로 출력되는 것\n",
        "\n",
        "입력 시퀀스에 대해서 원-핫 인코딩을 수행하고, 입력 데이터와 레이블 데이터를 텐서로 변환"
      ],
      "metadata": {
        "id": "9ejZZ8gvjejL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_one_hot = [np.eye(dic_size)[x] for x in x_data] # x 데이터는 원-핫 인코딩\n",
        "X = torch.FloatTensor(x_one_hot)\n",
        "Y = torch.LongTensor(y_data)"
      ],
      "metadata": {
        "id": "r0e-7Vs3jsFs"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "훈련 데이터와 레이블 데이터 크기를 확인하기"
      ],
      "metadata": {
        "id": "y6-MbP5IjtNq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('훈련 데이터의 크기 : {}'.format(X.shape))\n",
        "print('레이블의 크기 : {}'.format(Y.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eprfkLV6jvI_",
        "outputId": "efc5c95d-2096-42c4-8700-2e6ea8ae7169"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련 데이터의 크기 : torch.Size([170, 10, 25])\n",
            "레이블의 크기 : torch.Size([170, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "원-핫 인코딩 된 결과를 보기 위해서 첫 번째 샘플만 출력하기"
      ],
      "metadata": {
        "id": "WHd3Cn4qjwMf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(X[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4kopRsojzUs",
        "outputId": "2c2276b7-e38e-4d04-829a-0e36ecb27da2"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 1., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         1., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "레이블 데이터의 첫 번째 샘플도 출력"
      ],
      "metadata": {
        "id": "MAfyZhM4j0qH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(Y[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6gW6zjtoj2TX",
        "outputId": "3d8b2946-10f5-4ea6-c248-1a701aafad00"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([18,  2,  8, 17, 11,  2,  3,  5, 10, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "f you want에 해당됨"
      ],
      "metadata": {
        "id": "C6t7JmqHj3a9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2) 모델 구현하기"
      ],
      "metadata": {
        "id": "O05jA7-Oj6jr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "은닉층을 두 개 쌓기"
      ],
      "metadata": {
        "id": "VKma2dX7j8ZU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, layers): # 현재 hidden_size는 dic_size와 같음.\n",
        "        super(Net, self).__init__()\n",
        "        self.rnn = torch.nn.RNN(input_dim, hidden_dim, num_layers=layers, batch_first=True)\n",
        "        self.fc = torch.nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, _status = self.rnn(x)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "8wZf5_8Aj-XQ"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = Net(dic_size, hidden_size, 2) # 이번에는 층을 두 개 쌓습니다."
      ],
      "metadata": {
        "id": "A9Ms7OMAj_wU"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "nn.RNN() 안에 num_layers라는 인자를 사용"
      ],
      "metadata": {
        "id": "MA4-EsjtkBOx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), learning_rate)"
      ],
      "metadata": {
        "id": "YGUiu3YekF-B"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델에 입력을 넣어서 출력의 크기를 확인하기"
      ],
      "metadata": {
        "id": "ilSgrQpAkHLB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = net(X)\n",
        "print(outputs.shape) # 3차원 텐서"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvJVxCeDkLfd",
        "outputId": "0a130b84-bf74-4024-c904-b476be1729c8"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([170, 10, 25])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(170, 10, 25) 의 크기를 가지는데 각각 배치 차원, 시점, 출력의 크기"
      ],
      "metadata": {
        "id": "9YRCMXmykMnP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(outputs.view(-1, dic_size).shape) # 2차원 텐서로 변환."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YgFDFp0LkQy5",
        "outputId": "21493502-05e8-4ed8-9e26-5455707debd3"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1700, 25])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "레이블 데이터의 크기를 다시 복습"
      ],
      "metadata": {
        "id": "ioD8nKZ9kR52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(Y.shape)\n",
        "print(Y.view(-1).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSxDatmvkTxu",
        "outputId": "25d19698-a55b-40fe-a77e-f83fd296e872"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([170, 10])\n",
            "torch.Size([1700])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "옵티마이저와 손실 함수를 정의"
      ],
      "metadata": {
        "id": "5q0Q9XaWkU_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(100):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = net(X) # (170, 10, 25) 크기를 가진 텐서를 매 에포크마다 모델의 입력으로 사용\n",
        "    loss = criterion(outputs.view(-1, dic_size), Y.view(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # results의 텐서 크기는 (170, 10)\n",
        "    results = outputs.argmax(dim=2)\n",
        "    predict_str = \"\"\n",
        "    for j, result in enumerate(results):\n",
        "        if j == 0: # 처음에는 예측 결과를 전부 가져오지만\n",
        "            predict_str += ''.join([char_set[t] for t in result])\n",
        "        else: # 그 다음에는 마지막 글자만 반복 추가\n",
        "            predict_str += char_set[result[-1]]\n",
        "\n",
        "    print(predict_str)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i46hu4qKkZRM",
        "outputId": "a142f54e-a152-4a07-e21e-53baff5db16e"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "' 'aii'swrisary'i'iayias'iaiisisaayaaisiisiaias'aiarsiiriasiyys'iiaaasiaisyirsisasaasaasssaiiriaiiiasisirsasaasiiiiasarrraasiiiasiaiiaiiiy'aasiiaayiiiaiiiiiassiaaisi'a'saisayiaiii\n",
            "                                                                                                                                                                                   \n",
            "                                                                                                                                                                                   \n",
            "s'.'hb.'hahaaraaaaannalaaaladarlhnaahhanaaaahnh aaalahaaahnaaahan nnnaa'rhaarhnarl naa hhnaahahhnahaaanhdaaahahaarhananhhaaaaahhanhhahnarahan haaanaaadaaahaahaalnnnahhaahhnaab haa\n",
            "srtnf'tnf'brb'rrrb'rrr'rrr''frbrbr''frb''rrrbf'frrrr'r'''rrr'fbrfrfrr'rr'rrr'rrr''frb'frf''rbrbrf'f'rrfrfrbrfrfrb'frb'frfr'rrrfbrfrfrbrb'bbrbrfrr'rr''frf'frrrrrr'br'bbr'ffr'w'rbr'\n",
            "thdoshtnd dhh   hd hhh h h  shd hhd dhd lhh hs d     hl   hh dhhi dhhlh    h hhhdld d dhs lhh hhd d hhilshdhs shdldhd t dh  hhdhhdhdhhhh dhhd dhhl hi dhd dhh hh ls  hh ldd  d hh  \n",
            "totoh thitii tiiiitii titissiiitmittniittnit it ittisitttni tnhit iiititsit iii ttiiitiimst iii ioisi thmii mim itmiioithitt i m thiiih tiiithh itiittmiioi itmi  mt hhithiiaiti it\n",
            "t t  ut stsuttttuu uusttttttutstsut su tt ut utstttttsttt ust stt suutttttt tsusttutu sustt stu s stutt sutsststuosut utstttstssut suussttuut ssuttttouuu ssttst  st s tts so tuutt\n",
            "t t stt tt  ttt       tottto   ts t    t   t  tsttot   tt   t stt    t t tt   t t            t       tt s t s   t s t  t t t t  tt      t   t s t tot t  o  t st   t s to  s  ottt \n",
            "t t   t  t   t        t t t       t    t         t     t    t o t    t tt t e   t         t           t r   r     s t            t      t   t       t         o    t    t          \n",
            "e   t  e     t        t t       r t  o  oo  o  o     r   o  too e    t t   oe   e    e    r        o    r   r r   r     e          e  o t   e r     t r       o oe  or   e roeo    \n",
            "e   ed e  e    dddo d   i   e   r roeeeroeddod edho  t   e  tetd oe        oi t   t  etee t  d eeoeod  ot   t t   r  o hed r     eoed o eeddr r      or d e   teeee ero  eotoe d d \n",
            "d tot to iot etdd t t r t  ot eet roeooreod od o ro oth oee rot  oee r    tot t  ot eetot to   eeoeo  rot tot to ot  o hedot eot roe  t  o erotoe t  ote heo htdottioto ototoeot   \n",
            "d tot to tot t   tt t r t eot d t roeoortod o  o rot theooe eot  oet t  t tot d   t toeot too  otoeo  dot tot totot eotho  t  ot doe  t  n et tod toeotoe eo  t otteoto  totoett  e\n",
            "d tot t  t thtt t e t t t t t ttt toe ot ottot e totette  t t t t e  th tot t t ttt t eot t    otoeo  t t toe t t t n e ot t t t t e  o tt  t t t toe o   t  tt oete t  ntet ett te\n",
            "t thttt  t thtt ttt t t t t t tttot    t  ttottt totettee t t ttt    thet t t t tet t t tnt  e  toe t tod toe t t t t t et t t t t e  t tettt t t t et  t t e theett t  etet eet t \n",
            "t tht t  t t t  t d t t t   t t t toet t  te   e tht th t t tdottn   t tt t t t ttt t t o de es shtss thd t t t tei t thoe t t e thet tottett tot to tt d d  aa n nt d  nset et  t \n",
            "t tht to t t es t d t t ts  t t t toet toetsns e totethed toetot esd t gt t t tottd dotmd tdoes shdss t d wos t ted t thos totso thes totdsrtetod to dt d t   t e dteto  ded e   s \n",
            "t thr to  ot nsed t tot eseet d ththeg teeson ee totethed to todlesd t st tot totlt dos e ttoedesheeo t d toe t ted t s es toeso thes ton ostesod th dt d e s t   eteto od toe s e \n",
            "e thd to tot    r t totoestot d ththeb teese  ee totet ed toulosle d t et ted toult te  oettoedeshe t t d toe toded t thes toest thee touto tetod to  t d eoe t   et to od toe s et\n",
            "e thdlto c th ' elt towoemtotod tht em tdeme  ee touet ed touconle t thet ted tould te  tettoe   oe t t d toe t red t them toest thee to  o tet d to  t d eoe t   et to od toe s et\n",
            "e todlto t thu em t sowot totodltht eb tdeme uee touet ed tou oe e r thet ted toucd te weettoe  toe e t d toe t ted t them toest thee to  o tlt d to  cod eo htoe ee to ee toe t et\n",
            "e thdltoet to  enet t woemt doo't t  botteee tle  he t ed toecod e t th t tet toult tdo't ttoe  thect tot toe t ted t dhe  toedt thee to co t tor the tod toehto  et to eoothe toe \n",
            "e th pto t toe ennt t t emt doo't t tlotleme dle thelt ed toecod e toto t tet totlt tm ot tth c thece tot toe t ted tcdhem toedo them to to t ton the tod toshtl  en to oo theeto l\n",
            "e thdlto t to cnd t totoemt don t t em tmose tl ttoe t ed to codle t to t tlt toaid tm ,e tto   theeo t d toett ted t shem toedo toem toulo tlt   toeccod e thtm e t t  o  toe aorl\n",
            "e th lto t toe edst t toimt toa't t e'  'ote tle th  theb toelod eet to t tld toald t'soo tto   theco t t to  t ted t thnm to  t them toe e t t d toe tod tothtm   t toaee themthrl\n",
            "e to ltoet t elndlt t toemt t a't t em um te dle to  thed toecod'e toto d ted t ald t'eht tto e theco t t toe t tud a them toe t them toe o tet d toe t d t thtm  nt toaua themlorl\n",
            "d tonlto t to bndlt t themt doe't t  m te se dle ton thet to bo 't t thod t d toald t eht tto m theco t d toelt tud a shem toe o toem toalo let e toe t d toshtm  ns toaooetoemtorl\n",
            "d tonltoet toabndlt aothem, dhe'd t  o ue se dle ton them toabo le t thrt t d toald t set ttonm thect t d aoelt aud a shem to co toem to tonlltoe toe tod t shtm  nd to eo toe torl\n",
            "d torltont toebuded aothem, a e't to o  e te dle toe thed to bod'l t ahr, ted toa'thtsshtnttonm thect bud aoelt tud aoshem to co them to bonlltoe toe t d a shtm  ne to eo toessorl\n",
            "d tormcont toebudld aethem, ahe't doem  e te gle tor them toecod'l t ao , t d doa'thtssedmttoem thest b d aorkt tud toshem to lh them toeconllt   toe t d e shdm ert toaes toecworl\n",
            "u torlcont toecudld anthem, ahe't toem up teogle tor ther to cod'nst the, tnd toa't tssednttoem thect b d dorct aud a sher to co them to conllt r to tt d e shdm ent ty es toercorl\n",
            "u tonltont to cndld anbhet, d r't t em up teogle tor ther to conl  t whod tnd torlt tsshpnttoem toect b d torkp aud a sher to co them to co lltoe toe t d e shdm ent ty eo toe corl\n",
            "u ton'tont torcudsd a shit, aor't tou, up teople tor ther to conl  t bhod t d torlt tssopnttoem tonkt a d tor p sud aothem torlo toem to co tltor toe sod e s dm  nt ty em toemcorl\n",
            "u ton'tont toncnttd a them, dhe't toe, up teodle tor ther to conlnst ahod t d tor't gsshinttoem togki t d tookn aud aosher to ch toem to co tltor toe t d e s im ent py em toemcnrc\n",
            "g ton'tont aoncuttd anthem, ahr't toe, up teople tor ther to conlest ahod tnd tor't tsshpnttoem torkn t d to kn bud a sher torch woem to contlton toemtod e shbm ent ty em toemcnrc\n",
            "g won'tont ao budld a thim, d n't toe, up perple to lther to co lestyahod t d ton't tsshpnttoem toskn t d torkp bud d ther torch them to conllton the thd esshim entity em toemcndc\n",
            "g won'tont ao butld a shim, d n't toup up perple to lther to co lect ahod t d ton't tss sstthem toskn t d tookn bud t ther torct toem to co plton toe tod esshim entity em toe sndc\n",
            "g won cont do butl, a shim, dhn't toum up perple to lther to co lect wood tnd ton't tssnistthem toskn t d dookn but t ther to ct toem to conplton toemt d esshim entity om toemc dc\n",
            "g won tont do butld a ship, dhn't toum up terple to  ther to codlect wood dnd don't tssngstthem tosks and dookn but d ther to ch them to conllfon aoemsoi esshim ensity om toemcndc\n",
            "g won tont to butld a ship, ahn't toum up teople thr ther to codlect wood and don't tsshgstthem toskt and dookn but t ther toach toem to co g won toemsod esship entity of aoemsndc\n",
            "g won'th t to butld a ship, a n'thtoul up people tou ther to coll ct wood and to 't tsshgntthem toskt tnd dorks but tasher toach toem to co p fon themsod esship eicity of toemsndc\n",
            "g won'cm t to build anship, bon't toup up people tou ther to collest wood and do 't dssigntthem tosks and dorko bud tather toach toem to co g fon toemsodlesship emtity of toemsndc\n",
            "g won'thnt to budld a ship, bon't tauk up people tog ther to collect wond and don't dssigntthem tosks and work, bud tather toach them to co g ton themsnd esship endity of uoemcndc\n",
            "g aon'thnt ro budld a ship, bon't tauk up people tor ther to collect wo d and don't dssigntthem tosks and work, bud rather toach ther to long ton themsnd esship eniity of toemcndc\n",
            "g aonktont ro build a ship, bon't taup up people tor ther to collect wo d and do 't wsshgntthem toskn and dorko but rather toach ther taglong tor themsndlesship e sity of themcndc\n",
            "g pon tont do bunld a shim, don't toup up people threther to collect woop and don't tssigntthem tasks and dork, bud rather toach them ta co g tor themsndlesship   iity or themsndc\n",
            "m won thnt ro build a shim, don't aouk up pendle thg ther to collect wood and don't assiintthem tosks and aor , bud rather toach them to lo g for themsnd eeshim ens ty of uhemsntc\n",
            "m won tont ro build a ship, don't doum up people thgether to collect wood and aon't dssignsthem tosks and aork, but rather toach them to long tor themsnd esshim ens ty of uhemsntc\n",
            "m won thnt ro butld a ship, don't doum up people thgether to collect wood ans won't dssignsthem tosks and dork, but rather toach the  to long for themend esshimmensity of themsntc\n",
            "f won sant to butld a ship, don't doum up people thg ther to bollect wood and don't dssignsthem tosks and dork, but rather toach the  ta long tor themend esshim ensity of themsnrc\n",
            "f won  ant to butld a ship, don't doum up people thgether to collect wood and don't dssign them tosks and dork, but rather toach them to long for themend esshim ensity of themsnrc\n",
            "f fon  ant to build a ship, don't dyum up people thtether to collect wood and don't dssign them tosks and dook, but rather toach them to long for themend esshim ensity of themsnrc\n",
            "f fon  ant to butld a ship, don't dyum up people toge her to collect wood and don't dssign them tosks and dork, but rather toach them to long for themendless immensity of themsnrc\n",
            "f fon  ant to budld anship, don't arum up people together to collect wood and won't assign them tosks and work, but rather toach them to long for themsnd ess ammensity of themsnrc\n",
            "g fon  ant to build a ship, don't arum up people tog ther to collect wood and won't assign them tosks and work, but rather toach them to long for themendless immensity of themsnrc\n",
            "g fon  ant to build a ship, don't doum up people together to collect wood and don't assign them tosks and dork, but rather toach them to long for them ndless immensity of themsnrc\n",
            "g fon want to butld a ship, don't doum up people together to collect wood and don't dssign them tosks and dork, but rather toach them to long for themendless immensity of themsnrl\n",
            "m yon want to butld a ship, don't doum up people together to collect wood and don't dssign them tasks and dork, but rather toach them to long for themendless immensity of themsnrc\n",
            "m you want to butld a ship, don't doum up people together to collect wood and don't dssign them tasks and dork, but rather toach them to long for themendless immensity of themsnrc\n",
            "m you want to build a ship, don't doum up people together to collect wood and don't assign them tasks and dork, but rather toach them ta long for themendless immensity of themsnac\n",
            "m you want to butld a ship, don't drum up people together to collect wood and don't assign them tasks and dork, but rather toach them ta long for the endless immensity of the enal\n",
            "t you want to butld a ship, don't drum up people together to collect wood and don't dssign the  tasks and dork, but rather toach them ta long for the endless immensity of the enal\n",
            "t you want to build a ship, don't drum up people together to collect wood and don't assign the  tasks and dork, but rather toach them to long for the endless immensity of the enal\n",
            "m you want to build a ship, don't drum up people together to collect wood and don't assign the  tasks and dork, but rather toach them to long for the endless immensity of the enal\n",
            "m fou want to build a ship, don't drum up people together to collect wood and don't assign the  tasks and dork, but rather toach them to long for the endless immensity of the eeal\n",
            "m fou want to build a ship, don't drum up people together to collect wood and don't assign the  tasks and dork, but rather toach them to long for the endless immensity of the eeal\n",
            "m fou want to build a ship, don't drum up people together to collect wood and don't assign the  tasks and dork, but rather toach them to long for the endless immensity of the eeal\n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and dork, but rather toach them to long for the endless immensity of the eeal\n",
            "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and dork, but rather toach them to long for the endless immensity of the seal\n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and dork, but rather toach them to long for the endless immensity of the seal\n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and dork, but rather toach them to long for the endless immensity of the seal\n",
            "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and dork, but rather toach them to long for the endless immensity of the sea.\n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them to long for the endless immensity of the sea.\n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and dork, but rather toach them to long for the endless immensity of the sea.\n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them to long for the endless immensity of the sea.\n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them to long for the endless immensity of the eea.\n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them to long for the endless immensity of the eea.\n",
            "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them to long for the endless immensity of the eea.\n",
            "t you want to build a ship, don't drum up people together to collect wood and won't assign them tasks and work, but rather toach them to long for the endless immensity of the eea.\n",
            "t you want to build a ship, don't drum up people together to collect wood and won't assign them tasks and work, but rather toach them to long for the endless immensity of the eea.\n",
            "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them to long for the endless immensity of the eea.\n",
            "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them to long for the endless immensity of the eea.\n",
            "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them to long for the endless immensity of the sea.\n",
            "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them to long for the endless immensity of the sea.\n",
            "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them to long for the endless immensity of the sea.\n",
            "m you want to cudld a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them to long for the endless immensity of the sea.\n",
            "p you want to build a ship, don't drum up people together to collect wood and won't assign them tasks and work, but rather toach them to long for the endless immensity of the sea.\n",
            "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them to long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them to long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them to long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them to long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them to long for the endless immensity of the sea.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> 마지막 에포크에서는 꽤 정확한 문자를 생성함"
      ],
      "metadata": {
        "id": "Pxqn1Xx-kbNd"
      }
    }
  ]
}